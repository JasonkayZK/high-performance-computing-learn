{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasonkayZK/high-performance-computing-learn/blob/main/cuda/0-an-even-easier-intro-to-cuda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **一个简单的CUDA介绍**\n",
        "\n",
        "> **参考文章：**\n",
        ">\n",
        "> - https://developer.nvidia.com/blog/even-easier-introduction-cuda/\n",
        "> - https://colab.research.google.com/github/NVDLI/notebooks/blob/master/even-easier-cuda/An_Even_Easier_Introduction_to_CUDA.ipynb\n",
        "> - https://beta.infinitensor.com/camp/summer2025/stage/1/course/cuda-programming?tab=1\n",
        "> - https://qiankunli.github.io/2025/03/22/cuda.html\n"
      ],
      "metadata": {
        "id": "_creanvHPsDq"
      },
      "id": "_creanvHPsDq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "CUDA 是 NVIDIA 流行的并行计算平台和编程模型。\n",
        "\n",
        "CUDA C++是使用 CUDA 创建大规模并行应用程序的方法之一。它允许你使用 C++ 编程语言来开发高性能算法，这些算法由在 GPU 上运行的数千个并行线程加速。"
      ],
      "metadata": {
        "id": "qKLQ9kllP87S"
      },
      "id": "qKLQ9kllP87S"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **一、从一个数组加法开始**\n",
        "\n",
        "我们将从一个简单的 C++ 程序开始，该程序添加两个数组的元素，每个数组有一百万个元素。\n"
      ],
      "metadata": {
        "id": "0IWRAnPWQeiq"
      },
      "id": "0IWRAnPWQeiq"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile add.cpp\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// function to add the elements of two arrays\n",
        "void add(int n, float *x, float *y) {\n",
        "  for (int i = 0; i < n; i++)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void) {\n",
        "  int N = 1<<20; // 1M elements\n",
        "\n",
        "  float *x = new float[N];\n",
        "  float *y = new float[N];\n",
        "\n",
        "  // initialize x and y arrays on the host(CPU)\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the CPU\n",
        "  add(N, x, y);\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  delete [] x;\n",
        "  delete [] y;\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZhuvPJVRA05",
        "outputId": "f7dd613d-9c07-40a9-db04-149119aaeeef"
      },
      "id": "oZhuvPJVRA05",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting add.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> `%%writefile` 命令用于写入文件；\n",
        ">\n",
        "> 执行上述单元格会将其内容保存到文件 add.cpp。"
      ],
      "metadata": {
        "id": "KCEghg2uUHFF"
      },
      "id": "KCEghg2uUHFF"
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# 编译\n",
        "g++ add.cpp -o add\n",
        "\n",
        "# 运行\n",
        "./add"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0j-J8XzUWt2",
        "outputId": "84adcca7-ee37-4b6e-cce2-c87dda041f0c"
      },
      "id": "z0j-J8XzUWt2",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max error: 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "正如预期的那样，上面的代码打印求和中没有错误，然后退出。\n",
        "\n",
        "现在让这个计算在 GPU 的许多内核上（并行）运行。\n",
        "\n",
        "首先，只需要将我们的 add 函数变成 GPU 可以运行的函数，在 CUDA 中称为内核(kernel)。\n",
        "\n",
        "为此所要做的就是：将说明符 `__global__` 添加到函数中：**它告诉 CUDA C++编译器这是一个在 GPU 上运行的函数，可以从 CPU 代码调用：**\n",
        "\n",
        "```c\n",
        "// CUDA Kernel function to add the elements of two arrays on the GPU\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "```\n",
        "\n",
        "这些 `__global__` 函数称为内核(kernel)，在 GPU 上运行的代码通常称为设备代码(device code)，而在 CPU 上运行的代码是主机代码(host code)。"
      ],
      "metadata": {
        "id": "NbdbbyD-UthO"
      },
      "id": "NbdbbyD-UthO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **二、CUDA 中的内存分配**\n",
        "\n",
        "要在 GPU 上进行计算，需要分配 GPU 可访问的内存。\n",
        "\n",
        "CUDA 中的**[统一内存](https://developer.nvidia.com/blog/unified-memory-in-cuda-6/)(Unified Memory)**通过提供系统中所有 GPU 和 CPU 可访问的单个内存空间来实现内存分配。\n",
        "\n",
        "要在统一内存中分配数据，需要调用 `cudaMallocManaged()`， 它返回一个指针；\n",
        "\n",
        "可以从主机（CPU）代码或设备（GPU）代码访问该指针。\n",
        "\n",
        "要释放数据，只需将指针传递给 `cudaFree()` 即可。\n",
        "\n",
        "只需要将上面代码中对 new 的调用替换为对 `cudaMallocManaged()` 的调用，并将对 `delete []` 的调用替换为对 `cudaFree()` 的调用！\n",
        "\n",
        "```c\n",
        "  // Allocate Unified Memory -- accessible from CPU or GPU\n",
        "  float *x, *y;\n",
        "  cudaMallocManaged(&x, N*sizeof(float));\n",
        "  cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "  ...\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "```\n",
        "\n",
        "最后，需要启动 `add()` 内核，在 GPU 上调用它；\n",
        "\n",
        "CUDA 内核启动使用：**三重尖括号语法** `<<< >>>` 指定。\n",
        "\n",
        "只需要将其添加到参数列表之前的调用中即可：\n",
        "\n",
        "```c\n",
        "add<<<1, 1>>>(N, x, y);\n",
        "```\n",
        "\n",
        "> 后续会详细介绍尖括号内的内容；\n",
        ">\n",
        "> 现在你需要知道的是：这一行启动了一个 GPU 线程来运行 `add()`\n",
        "\n",
        "还有一个问题：需要 CPU 等到内核函数 add 完成后再访问结果（**因为 CUDA 内核启动不会阻止调用的 CPU 线程**）！\n",
        "\n",
        "为此，只需在对 CPU 进行最终错误检查之前调用 `cudaDeviceSynchronize()`！\n",
        "\n",
        "下面是完整的代码："
      ],
      "metadata": {
        "id": "E_wHgrzXVwtz"
      },
      "id": "E_wHgrzXVwtz"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile add.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "// Kernel function to add the elements of two arrays\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "    y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20\n",
        " ;\n",
        "  float *x, *y;\n",
        "\n",
        "  // Allocate Unified Memory – accessible from CPU or GPU\n",
        "  cudaMallocManaged(&x, N*sizeof(float));\n",
        "  cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the GPU\n",
        "  add<<<1, 1>>>(N, x, y);\n",
        "\n",
        "  // Wait for GPU to finish before accessing on host\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi7LfU42XztV",
        "outputId": "1ce715e1-a99e-4140-b64f-717cda8752c3"
      },
      "id": "Gi7LfU42XztV",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting add.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvcc add.cu -o add_cuda\n",
        "./add_cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUOFwVM-X8UP",
        "outputId": "fffd4f50-f324-4971-8506-99a4a8362b84"
      },
      "id": "XUOFwVM-X8UP",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max error: 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "通过 `nvcc` 可以编译 CUDA 程序；\n",
        "\n",
        "而在编写代码时需要将 `*.cpp` 文件类型修改为 `*.cu` 类型表示是一个 CUDA 程序！"
      ],
      "metadata": {
        "id": "zbIPjZZFY2wv"
      },
      "id": "zbIPjZZFY2wv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **三、性能测试**\n",
        "\n",
        "找出内核函数运行需要多长时间的最简单方法是使用：`nvprof` 运行：\n",
        "\n",
        "nvprof 是 CUDA Toolkit 附带的命令行 GPU 分析器，只需在命令行上键入：\n",
        "\n",
        "`nvprof ./add_cuda`"
      ],
      "metadata": {
        "id": "pAK7rKlvZSut"
      },
      "id": "pAK7rKlvZSut"
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvprof ./add_cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXhioW_iaP9X",
        "outputId": "2dedf2f6-69f1-4ebc-e5b0-d48bd0832edc"
      },
      "id": "VXhioW_iaP9X",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==19628== NVPROF is profiling process 19628, command: ./add_cuda\n",
            "Max error: 1\n",
            "==19628== Profiling application: ./add_cuda\n",
            "==19628== Profiling result:\n",
            "No kernels were profiled.\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            "      API calls:   92.19%  92.918ms         2  46.459ms  48.457us  92.869ms  cudaMallocManaged\n",
            "                    7.39%  7.4475ms         1  7.4475ms  7.4475ms  7.4475ms  cudaLaunchKernel\n",
            "                    0.27%  269.24us         2  134.62us  118.45us  150.79us  cudaFree\n",
            "                    0.12%  125.53us       114  1.1010us     105ns  51.627us  cuDeviceGetAttribute\n",
            "                    0.01%  10.498us         1  10.498us  10.498us  10.498us  cuDeviceGetName\n",
            "                    0.01%  8.1820us         1  8.1820us  8.1820us  8.1820us  cudaDeviceSynchronize\n",
            "                    0.01%  5.1080us         1  5.1080us  5.1080us  5.1080us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.4580us         3     486ns     115ns  1.1100us  cuDeviceGetCount\n",
            "                    0.00%     812ns         2     406ns     140ns     672ns  cuDeviceGet\n",
            "                    0.00%     577ns         1     577ns     577ns     577ns  cuModuleGetLoadingMode\n",
            "                    0.00%     310ns         1     310ns     310ns     310ns  cuDeviceTotalMem\n",
            "                    0.00%     220ns         1     220ns     220ns     220ns  cuDeviceGetUuid\n",
            "\n",
            "==19628== Unified Memory profiling result:\n",
            "Total CPU Page faults: 24\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 若要查看 Colab 分配给你的当前 GPU，可以运行 `nvidia-smi`，然后查看 Name 列\n",
        ">\n",
        "> 例如，你可能会看到 Tesla T4："
      ],
      "metadata": {
        "id": "EDD6_7CUbO81"
      },
      "id": "EDD6_7CUbO81"
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh44WrqLbbKJ",
        "outputId": "d9d65762-04c9-49cc-e71f-3c360c0ca3a8"
      },
      "id": "Rh44WrqLbbKJ",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jul 25 08:09:31 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0             27W /   70W |       0MiB /  15360MiB |      4%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "下面，让我们通过提升并行度让代码更快运行！"
      ],
      "metadata": {
        "id": "JdrnqgaFbjsM"
      },
      "id": "JdrnqgaFbjsM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **四、提高Threads**\n",
        "\n",
        "在刚才的实验中，我们已经运行了一个带有一个执行一些计算的线程的内核。那么如何让它并行呢？\n",
        "\n",
        "关键在于 CUDA 的 `<<<1、1>>>` 语法：这称为执行配置，它告诉 CUDA 运行时在 GPU 上启动时使用多少并行线程。\n",
        "\n",
        "这里有两个参数，先让我们从更改第二个参数开始：**线程块中的线程数**。\n",
        "\n",
        "**CUDA GPU 使用大小为 32 的倍数的线程块运行内核，因此 256 线程是一个合理的大小选择。**\n",
        "\n",
        "```c\n",
        "add<<<1, 256>>>(N, x, y);\n",
        "```\n",
        "\n",
        "但是如果只运行修改此处的代码，它将：**为每个线程都执行完整的循环计算，而不是将循环分散到并行线程中！**\n",
        "\n",
        "> **这是由于 GPU 是 SIMT 的执行方式！**\n",
        "\n",
        "为了正确地做到这一点，需要修改内核：CUDA C++ 提供了让内核获取正在运行的线程的索引的关键字。\n",
        "\n",
        "具体来说，threadIdx.x 包含其块内当前线程的索引，blockDim.x 包含块中的线程数；\n",
        "\n",
        "因此需要修改循环以使用并行线程在数组中以 stride 的长度前进；\n",
        "\n",
        "```c\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  int index = threadIdx.x;\n",
        "  int stride = blockDim.x;\n",
        "  for (int i = index; i < n; i += stride)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "```\n",
        "\n",
        "add 函数没有太大变化，只是：**将循环的任务分配到了不同的 thread 中！**\n",
        "\n",
        "> 事实上，将 index 设置为 0 并将 stride 设置为 1 在语义上与第一个版本相同！\n",
        "\n",
        "将文件保存为 add_block.cu，并在 nvprof 中再次编译和运行："
      ],
      "metadata": {
        "id": "UxzOucWzbpMw"
      },
      "id": "UxzOucWzbpMw"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile add_block.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// Kernel function to add the elements of two arrays\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  int index = threadIdx.x;\n",
        "  int stride = blockDim.x;\n",
        "  for (int i = index; i < n; i += stride)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20;\n",
        "  float *x, *y;\n",
        "\n",
        "  // Allocate Unified Memory – accessible from CPU or GPU\n",
        "  cudaMallocManaged(&x, N*sizeof(float));\n",
        "  cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the GPU\n",
        "  add<<<1, 256>>>(N, x, y);\n",
        "\n",
        "  // Wait for GPU to finish before accessing on host\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z-uDdxVeyyf",
        "outputId": "c070a138-74ac-4363-a214-87435ade25ac"
      },
      "id": "-Z-uDdxVeyyf",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting add_block.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvcc add_block.cu -o add_block\n",
        "nvprof ./add_block"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydOIuWH_e4cC",
        "outputId": "575b938b-59e0-4fea-a10f-13738d4c2fae"
      },
      "id": "ydOIuWH_e4cC",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==19696== NVPROF is profiling process 19696, command: ./add_block\n",
            "Max error: 1\n",
            "==19696== Profiling application: ./add_block\n",
            "==19696== Profiling result:\n",
            "No kernels were profiled.\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            "      API calls:   93.94%  122.13ms         2  61.064ms  58.832us  122.07ms  cudaMallocManaged\n",
            "                    5.72%  7.4305ms         1  7.4305ms  7.4305ms  7.4305ms  cudaLaunchKernel\n",
            "                    0.22%  279.79us         2  139.90us  122.77us  157.02us  cudaFree\n",
            "                    0.10%  136.08us       114  1.1930us     105ns  57.117us  cuDeviceGetAttribute\n",
            "                    0.01%  11.251us         1  11.251us  11.251us  11.251us  cuDeviceGetName\n",
            "                    0.01%  10.418us         1  10.418us  10.418us  10.418us  cudaDeviceSynchronize\n",
            "                    0.01%  8.0560us         1  8.0560us  8.0560us  8.0560us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.6270us         3     542ns     121ns  1.2840us  cuDeviceGetCount\n",
            "                    0.00%     778ns         2     389ns     163ns     615ns  cuDeviceGet\n",
            "                    0.00%     694ns         1     694ns     694ns     694ns  cuModuleGetLoadingMode\n",
            "                    0.00%     419ns         1     419ns     419ns     419ns  cuDeviceTotalMem\n",
            "                    0.00%     299ns         1     299ns     299ns     299ns  cuDeviceGetUuid\n",
            "\n",
            "==19696== Unified Memory profiling result:\n",
            "Total CPU Page faults: 24\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "通过查看 GPU 活动字段来比较添加内核的时间：可以看到这是一个很大的加速！\n",
        "\n",
        "让我们继续获得更多的性能！"
      ],
      "metadata": {
        "id": "E2hS8CmrfNM9"
      },
      "id": "E2hS8CmrfNM9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **五、多个Block**\n",
        "\n",
        "CUDA GPU 都具有许多并行处理器，这些处理器被称为流式多处理器(SM, Streaming Multiprocessor)。每个 SM 可以运行多个并发线程块。\n",
        "\n",
        "例如，基于 [Pascal GPU 架构](https://developer.nvidia.com/blog/inside-pascal/)的 Tesla P100 GPU 有 56 个 SM，每个 SM 能够支持多达 2048 个活动线程。\n",
        "\n",
        "为了充分利用所有这些线程，应该启动具有多个线程块的内核！\n",
        "\n",
        "> **类似于每个CPU核中都可以执行2048个线程！**\n",
        "\n",
        "而执行配置中的：**第一个参数指定了线程块的数量！**这些并行线程块共同构成了所谓的网格。\n",
        "\n",
        "对于 N 个元素要处理，每个块有 256 个线程，只需要计算块数即可获得 N 个线程。\n",
        "\n",
        "将 N 除以块大小（如果 N 不是 blockSize 的倍数，需要小心四舍五入！）\n",
        "\n",
        "```c\n",
        "int blockSize = 256;\n",
        "int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "add<<<numBlocks, blockSize>>>(N, x, y);\n",
        "```\n",
        "\n",
        "![cuda_indexing](https://developer-blogs.nvidia.com/wp-content/uploads/2017/01/cuda_indexing.png)\n",
        "\n",
        "> 上图说明了使用 blockDim.x、gridDim.x 和 threadIdx.x 在 CUDA 中索引到数组（一维）的方法！\n",
        "\n",
        "还需要更新内核代码以考虑线程块所在的网格：\n",
        "\n",
        "CUDA 提供了 gridDim.x，其中包含网格中的块数，以及 blockIdx.x，参数包含了网格中当前线程块的索引；\n",
        "\n",
        "思路是：\n",
        "\n",
        "- 每个线程通过计算到其块开头的偏移量（块索引乘以块大小：blockIdx.x * blockDim.x）\n",
        "- 并在块内添加线程的索引（threadIdx.x）来获取其索引\n",
        "\n",
        "这里的代码：`blockIdx.x * blockDim.x + threadIdx.x` 是惯用的技巧！\n",
        "\n",
        "```c\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "  for (int i = index; i < n; i += stride)\n",
        "    y[i] = x[i] + y[i];\n",
        "}\n",
        "```\n",
        "\n",
        "更新后的内核还设置了网格中的线程总数 （blockDim.x * gridDim.x）；\n",
        "\n",
        "CUDA 内核中这种类型的循环通常称为[网格步幅循环(grid-stride)](https://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/)。\n",
        "\n",
        "下面，将文件另存为 add_grid.cu，然后再次在 nvprof 中编译和运行它。"
      ],
      "metadata": {
        "id": "eZUyedCCfULT"
      },
      "id": "eZUyedCCfULT"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile add_grid.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// Kernel function to add the elements of two arrays\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "  for (int i = index; i < n; i += stride)\n",
        "    y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20;\n",
        "  float *x, *y;\n",
        "\n",
        "  // Allocate Unified Memory – accessible from CPU or GPU\n",
        "  cudaMallocManaged(&x, N*sizeof(float));\n",
        "  cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the GPU\n",
        "  int blockSize = 256;\n",
        "  int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "  add<<<numBlocks, blockSize>>>(N, x, y);\n",
        "\n",
        "  // Wait for GPU to finish before accessing on host\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "GK7E_xnXmK14",
        "outputId": "6eadd658-1c8e-4e3b-a8b0-969004510c81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "GK7E_xnXmK14",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting add_grid.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvcc add_grid.cu -o add_grid\n",
        "nvprof ./add_grid"
      ],
      "metadata": {
        "id": "oPYbAJZOmOQm",
        "outputId": "a40f2ad1-a8eb-4255-946b-cd7e7f0d29b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "oPYbAJZOmOQm",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==19751== NVPROF is profiling process 19751, command: ./add_grid\n",
            "Max error: 1\n",
            "==19751== Profiling application: ./add_grid\n",
            "==19751== Profiling result:\n",
            "No kernels were profiled.\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            "      API calls:   93.35%  113.28ms         2  56.639ms  44.803us  113.23ms  cudaMallocManaged\n",
            "                    6.20%  7.5263ms         1  7.5263ms  7.5263ms  7.5263ms  cudaLaunchKernel\n",
            "                    0.26%  315.41us         2  157.71us  130.85us  184.56us  cudaFree\n",
            "                    0.15%  186.99us       114  1.6400us     142ns  77.782us  cuDeviceGetAttribute\n",
            "                    0.02%  21.752us         1  21.752us  21.752us  21.752us  cuDeviceGetName\n",
            "                    0.01%  8.4250us         1  8.4250us  8.4250us  8.4250us  cudaDeviceSynchronize\n",
            "                    0.01%  6.9510us         1  6.9510us  6.9510us  6.9510us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.0900us         3     696ns     218ns  1.5980us  cuDeviceGetCount\n",
            "                    0.00%  1.5390us         2     769ns     225ns  1.3140us  cuDeviceGet\n",
            "                    0.00%     773ns         1     773ns     773ns     773ns  cuDeviceTotalMem\n",
            "                    0.00%     575ns         1     575ns     575ns     575ns  cuModuleGetLoadingMode\n",
            "                    0.00%     379ns         1     379ns     379ns     379ns  cuDeviceGetUuid\n",
            "\n",
            "==19751== Unified Memory profiling result:\n",
            "Total CPU Page faults: 24\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "这是运行多个块的巨大加速！"
      ],
      "metadata": {
        "id": "oBDzwt8cmYvL"
      },
      "id": "oBDzwt8cmYvL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **六、后续**\n",
        "\n",
        "### **1、提升内容**\n",
        "\n",
        "- 在内核中试验`printf()`。 尝试打印出部分或全部线程的 threadIdx.x 和 blockIdx.x 的值。他们是按顺序打印的吗？为什么或为什么不？\n",
        "- 在内核中打印 threadIdx.y 或 threadIdx.z（或 blockIdx.y）的值。（blockDim 和 gridDim 也是如此）。为什么存在这些？如何让它们采用 0 以外的值（1 表示暗淡）？\n",
        "\n",
        "\n",
        "### **2、后续阅读**\n",
        "\n",
        "-   [How to Implement Performance Metrics in CUDA C++](https://developer.nvidia.com/blog/how-implement-performance-metrics-cuda-cc/)\n",
        "-   [How to Query Device Properties and Handle Errors in CUDA C++](https://developer.nvidia.com/blog/how-query-device-properties-and-handle-errors-cuda-cc/)\n",
        "-   [How to Optimize Data Transfers in CUDA C++](https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/)\n",
        "-   [How to Overlap Data Transfers in CUDA C++](https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/)\n",
        "-   [How to Access Global Memory Efficiently in CUDA C++](https://developer.nvidia.com/blog/how-access-global-memory-efficiently-cuda-c-kernels/)\n",
        "-   [Using Shared Memory in CUDA C++](https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/)\n",
        "-   [An Efficient Matrix Transpose in CUDA C++](https://developer.nvidia.com/blog/efficient-matrix-transpose-cuda-cc/)\n",
        "-   [Finite Difference Methods in CUDA C++, Part 1](https://developer.nvidia.com/blog/finite-difference-methods-cuda-cc-part-1/)\n",
        "-   [Finite Difference Methods in CUDA C++, Part 2](https://developer.nvidia.com/blog/finite-difference-methods-cuda-c-part-2/)\n",
        "-   [Accelerated Ray Tracing in One Weekend with CUDA](https://developer.nvidia.com/blog/accelerated-ray-tracing-cuda/)\n",
        "\n",
        "对于刚接触CUDA的同学：\n",
        "\n",
        "- [_Fundamentals of Accelerated Computing with CUDA C/C++_](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-01+V1/about)：提供了专用的 GPU 资源、更复杂的编程环境。\n",
        "- [NVIDIA Nsight Systems™](https://developer.nvidia.com/nsight-systems)：可视化分析器的使用、数十个交互式练习、详细的演示、超过 8 小时的材料，以及获得 DLI 能力证书的能力。\n",
        "- [_Fundamentals of Accelerated Computing with CUDA Python_](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-02+V1/about)：Python 程序员阅读\n",
        "\n"
      ],
      "metadata": {
        "id": "dnoNrj--m36S"
      },
      "id": "dnoNrj--m36S"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}